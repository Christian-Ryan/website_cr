<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R | Christian Ryan</title>
    <link>https://drchristianryan.com/tags/r/</link>
      <atom:link href="https://drchristianryan.com/tags/r/index.xml" rel="self" type="application/rss+xml" />
    <description>R</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 15 Feb 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>R</title>
      <link>https://drchristianryan.com/tags/r/</link>
    </image>
    
    <item>
      <title>2.4 - Sentiment analysis of dreams</title>
      <link>https://drchristianryan.com/2020/02/15/sentiment-analysis-of-dreams/</link>
      <pubDate>Sat, 15 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://drchristianryan.com/2020/02/15/sentiment-analysis-of-dreams/</guid>
      <description>


&lt;p&gt;In the last post we compared the dream sets by graphing the most frequently occurring words and calculating correlation coefficients. But in psychology, we are often interested in specific aspects of the text to analyse. From my own perspective, emotional language use is of particular interest. A further way in which we could compare the dreams is by carrying out a sentiment analysis. One could use bespoke software such as the LIWC programme (&lt;a href=&#34;http://liwc.wpengine.com&#34; class=&#34;uri&#34;&gt;http://liwc.wpengine.com&lt;/a&gt;) in this scenario, but the tidytext approach allows for more flexibility, even if the dictionaries are not as well validated as LIWC.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://drchristianryan.com/post/2020-02-15-sentiment-analysis-of-dreams_files/LIWC.png&#34; style=&#34;width:45.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There used to be three sentiment dictionaries included in the &lt;strong&gt;tidytext&lt;/strong&gt; package and the current version of &lt;strong&gt;Text mining with R&lt;/strong&gt; (&lt;a href=&#34;https://www.tidytextmining.com&#34; class=&#34;uri&#34;&gt;https://www.tidytextmining.com&lt;/a&gt;) describes them this way. However, some changes have happened to the package, and now it comes with only the Bing set preloaded. To access the other lexicons, we need to install the &lt;strong&gt;textdata&lt;/strong&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;textdata&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we load it into the library and run the &lt;em&gt;lexicon_afinn()&lt;/em&gt; function. This creates a pop-up in the console which we need to respond to, confirming we wish to download the lexicon. Here I have stored it as a new tibble called &lt;em&gt;afinn&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(textdata)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;afinn &amp;lt;- lexicon_afinn()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s reload our data from the previous post.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(&amp;quot;df_word.Rdata&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we get started, we will load some other packages into the library that we might need.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidytext)
library(car)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can have a quick view of the lexicon by using the &lt;em&gt;some()&lt;/em&gt; function from the &lt;strong&gt;car&lt;/strong&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;some(afinn)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
##    word           value
##    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
##  1 doubts            -1
##  2 endorses           2
##  3 favored            2
##  4 foolish           -2
##  5 haplessness       -2
##  6 irritated         -3
##  7 misinformation    -2
##  8 sulky             -2
##  9 thwarts           -2
## 10 warm               1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we see the various ratings given for the valence of each word regarded as having sentiment. This is scored on a scale from -5 to +5.&lt;/p&gt;
&lt;div id=&#34;merging-sentiment-with-dream-dataset-via-inner_join&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Merging sentiment with dream dataset via inner_join()&lt;/h2&gt;
&lt;p&gt;As we have our original data as &lt;em&gt;df_word&lt;/em&gt; and the new dataframe &lt;em&gt;afinn&lt;/em&gt;, we can use the &lt;em&gt;inner_join()&lt;/em&gt; which will join the two datasets together, keeping only those rows that match. There is a &lt;em&gt;word&lt;/em&gt; variable in both datasets, so this value will be used automatically as the &lt;em&gt;by = &lt;/em&gt; argument for the join. We could have been explicit and written &lt;em&gt;inner_join(afinn, by = “word”)&lt;/em&gt;, but we know that both datasets contain a word variable so we can skip this step, and you will notice we get a message confirming this is how the join was made.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word &amp;lt;- df_word %&amp;gt;%
  inner_join(afinn)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;word&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,921 x 4
##    sample        dream_number word     value
##    &amp;lt;fct&amp;gt;                &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;
##  1 college_women            2 dream        1
##  2 college_women            3 failed      -2
##  3 college_women            4 careful      2
##  4 college_women            5 dream        1
##  5 college_women            5 straight     1
##  6 college_women            5 laughing     1
##  7 college_women            5 stopped     -1
##  8 college_women            6 angry       -3
##  9 college_women            6 dream        1
## 10 college_women            7 hard        -1
## # … with 1,911 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After matching words in the afinn sentiment lexicon with words in our dreams dataframe df_word, we can run a count to see the most frequently occurring words with sentiment ratings. If we also group_by the &lt;em&gt;word&lt;/em&gt; and &lt;em&gt;value&lt;/em&gt;, we can see the sentiment assigned to each word at the same time. We will use a trick I learned from David Robinson’s video &lt;em&gt;“Ten Tremendous Tricks in the Tidyverse”&lt;/em&gt; which is here.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=NDHSBUN_rVU&#34; class=&#34;uri&#34;&gt;https://www.youtube.com/watch?v=NDHSBUN_rVU&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;He points out that the &lt;em&gt;count()&lt;/em&gt; function has a &lt;em&gt;name&lt;/em&gt; argument so we can assign it a name rather than the default of “n”. Here we will call it &lt;em&gt;frequency&lt;/em&gt; as it is representing the frequency of words by token.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word %&amp;gt;%
  group_by(word, value) %&amp;gt;%
  count(word, sort = TRUE, name = &amp;quot;frequency&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 591 x 3
## # Groups:   word, value [591]
##    word      value frequency
##    &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;int&amp;gt;
##  1 dream         1       132
##  2 leave        -1        43
##  3 feeling       1        34
##  4 dead         -3        31
##  5 happy         3        28
##  6 top           2        28
##  7 stop         -1        27
##  8 beautiful     3        26
##  9 stopped      -1        24
## 10 afraid       -2        22
## # … with 581 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the word “dream” is the most frequently occurring word in the dataset with a sentiment rating. For a research study we would need to decide how to treat the word dream in this context. Should it be added as a custom stop-word? We might think of the valence rating as wrong in this context, as the &lt;em&gt;afinn&lt;/em&gt; dictionary is almost certainly giving a “1” value for the use of the word in common parlance in which it is used to refer to future plans - “I dream I will get married”; “I dream of winning the lottery”. These kind of wish fulfilment thoughts are probably mildly positive. However, the vast majority of uses of the word dream in this dataset is in a purely functional manner “I was dreaming that…”, which is probably neutral in terms of valence. We could regard occurrence of the word “dream” in the lexicon as an artefact when reporting the content of a dream, and justify removing it from the sentiment analysis. However, for simplicity, and as it only has a valence of ‘1’, we will leave it in.&lt;/p&gt;
&lt;p&gt;If we include the ‘sample’ as the first argument of the &lt;em&gt;count()&lt;/em&gt; function, we can retain this variable in the output. Then if we give our &lt;em&gt;group_by()&lt;/em&gt; function the arguments for word, value and dream_number, we can separate out the word and the sentiment (value) for each dream_number. The difference here is that we are counting the most frequent sentiment words &lt;em&gt;per dream&lt;/em&gt; rather than &lt;em&gt;per sample&lt;/em&gt;. We will come back to this issue of &lt;em&gt;level of analysis&lt;/em&gt; later in this post as we look at ways to aggregate the sentiment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word &amp;lt;- df_word %&amp;gt;%
  group_by(word, value, dream_number) %&amp;gt;%
  count(sample, word, sort = TRUE, name = &amp;quot;frequency&amp;quot;)
df_word&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,731 x 5
## # Groups:   word, value, dream_number [1,731]
##    word    value dream_number sample        frequency
##    &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;        &amp;lt;int&amp;gt; &amp;lt;fct&amp;gt;             &amp;lt;int&amp;gt;
##  1 crying     -2          305 hall_female           4
##  2 dead       -3          336 hall_female           4
##  3 dream       1          258 hall_male             4
##  4 hide       -1          202 hall_male             4
##  5 top         2          322 hall_female           4
##  6 war        -2          113 vietnam_vet           4
##  7 barrier    -2          173 vietnam_vet           3
##  8 blocks     -1           65 college_women         3
##  9 crying     -2          381 hall_female           3
## 10 dead       -3          351 hall_female           3
## # … with 1,721 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we see that ‘crying’ occurred 4 times in one dream (number 305), as did ‘dead’ and ‘war’.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;strongest-sentiments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Strongest sentiments&lt;/h2&gt;
&lt;p&gt;We could also look at arranging the data just by values. What are the most positive words that occurred in the dream dataset?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word %&amp;gt;%
  arrange(desc(value))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,731 x 5
## # Groups:   word, value, dream_number [1,731]
##    word      value dream_number sample      frequency
##    &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;        &amp;lt;int&amp;gt; &amp;lt;fct&amp;gt;           &amp;lt;int&amp;gt;
##  1 superb        5          157 vietnam_vet         1
##  2 brilliant     4          327 hall_female         1
##  3 brilliant     4          339 hall_female         1
##  4 fantastic     4          377 hall_female         1
##  5 fun           4          167 vietnam_vet         1
##  6 fun           4          390 hall_female         1
##  7 funny         4          177 vietnam_vet         1
##  8 funny         4          255 hall_male           1
##  9 overjoyed     4          280 hall_male           1
## 10 overjoyed     4          315 hall_female         1
## # … with 1,721 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Likewise, we can ask what are them most negative words in the dreams dataset?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word %&amp;gt;%
  arrange(value)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,731 x 5
## # Groups:   word, value, dream_number [1,731]
##    word    value dream_number sample      frequency
##    &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;        &amp;lt;int&amp;gt; &amp;lt;fct&amp;gt;           &amp;lt;int&amp;gt;
##  1 dick       -4          120 vietnam_vet         3
##  2 fucking    -4          170 vietnam_vet         2
##  3 ass        -4          107 vietnam_vet         1
##  4 fucker     -4          128 vietnam_vet         1
##  5 shit       -4          121 vietnam_vet         1
##  6 shit       -4          178 vietnam_vet         1
##  7 whore      -4          297 hall_male           1
##  8 dead       -3          336 hall_female         4
##  9 dead       -3          351 hall_female         3
## 10 die        -3          226 hall_male           3
## # … with 1,721 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This raises one of the key principles in computerised text analysis - human interpretation is crucial to checking the validity of the conclusions. Take the word ‘fucking’. If this is being used as an adjective, we might agree that it will probably indicate a negative appraisal of some event. However, it could be a verb! To check this we will need our original dataset where the dreams are still untokenized. Let’s read them in as &lt;em&gt;df&lt;/em&gt; then run the &lt;em&gt;str_which()&lt;/em&gt; function to identify the dream containing the word “fucking”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- read_csv(&amp;quot;dreams_df.csv&amp;quot;)
str_which(df$dream, pattern = &amp;quot;fucking&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 170&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can pull out dream 170 from the sample and take a closer look.&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;100%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;dream&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;I’m in a room filled with mostly men. On stage, my (dream) brother holds forth on matters of the day. From the back of the room I stand up. Holding up a small dark globe in my right hand, in a loud voice I say, “Tell me about this, buddy! Huh, tell me about this!” My brother freezes, astonished. I hold up a second globe. Gold letters spelling a word revolve in its hollow interior. “Or this, buddy! Tell me about this!” My brother is equally dumbstruck. Vigorously I say, “Tell you what, I’m gonna make you a deal. Tell the truth. Right now. Take this opportunity to tell the truth or I am gonna kill you.” After a pause, I say, “And when I say that, I mean figuratively. In a court of law, I’m gonna swat you with my hands and crush you.” My brother says, “Why should I talk?” I say, “Because you don’t have a fucking chance. You hear that. Not a fucking chance.” Cheering, the men in the room rise to their feet. All the while, we have lost sight of my other (dream) brother, Lewis, who is equally guilty. On stage, my (dream) brother begins talking, but it’s only a digression&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So we can be assured that the word was being used to describe negative sentiment and the lexicon classified it correctly in this instance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sentiment-metric&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sentiment metric&lt;/h2&gt;
&lt;p&gt;Now that we have assigned sentiment values to each word in our dataframe, it opens a possibility of generating a sentiment metric based on a combination of frequency (n) and valence (value). We could calculate this as &lt;em&gt;value&lt;/em&gt; x &lt;em&gt;frequency&lt;/em&gt;. We will add this new variable (sent_met) using a &lt;em&gt;mutate()&lt;/em&gt; function and then arrange the dataset by it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word &amp;lt;- df_word %&amp;gt;%
  mutate(sent_met = value * frequency) %&amp;gt;%
  arrange(sent_met)
df_word&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,731 x 6
## # Groups:   word, value, dream_number [1,731]
##    word    value dream_number sample      frequency sent_met
##    &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;        &amp;lt;int&amp;gt; &amp;lt;fct&amp;gt;           &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;
##  1 dead       -3          336 hall_female         4      -12
##  2 dick       -4          120 vietnam_vet         3      -12
##  3 dead       -3          351 hall_female         3       -9
##  4 die        -3          226 hall_male           3       -9
##  5 crying     -2          305 hall_female         4       -8
##  6 war        -2          113 vietnam_vet         4       -8
##  7 fucking    -4          170 vietnam_vet         2       -8
##  8 barrier    -2          173 vietnam_vet         3       -6
##  9 crying     -2          381 hall_female         3       -6
## 10 pain       -2          263 hall_male           3       -6
## # … with 1,721 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we can see the highest negative sentiment (per word) was the word ‘dead’ that appeared 4 times in a dream of one of the women in the hall_female sample, and with a sentiment of -3, resulted in the lowest value for the sent_met of -12. However, we have not aggregated sentiment &lt;em&gt;by dream&lt;/em&gt; yet. We could ask the question, on the basis of sentiment, what was the worst (or best) dream? But to answer this, we need to group the data by the dream_number and summarise the sent_met variable across words.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;split-apply-combine&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Split-Apply-Combine&lt;/h2&gt;
&lt;p&gt;There are a number of ways in which we can aggregate the data by dream, and we need to make a choice of the unit of analysis. One could argue for two levels of analysis here - the sentiment of each dreams and the sentiment of dreams by sample. One snag with this aggregation process across dreams is that we will be putting together negative and positive integers (for different words) with the risk of a high valenced dream (both good and bad) may have the sentiment values cancelling each other out. To avoid this, we can use a common data analytic strategy known as “split-apply-combine”. (see Wickham, H. (2011). The Split-Apply-Combine Strategy for Data Analysis. Journal of Statistical Software, 40(1). &lt;a href=&#34;https://doi.org/10.18637/jss.v040.i01&#34; class=&#34;uri&#34;&gt;https://doi.org/10.18637/jss.v040.i01&lt;/a&gt;). We can create two separate indices in each dataframe for positive and negative emotion words, and a composite sentiment score that adds their absolute value (R has an &lt;em&gt;abs()&lt;/em&gt; function which we can use for this purpose). The values need to be fed into separate dataframes to create the positive and negative values of sentiment. Later, we merge these two vectors back into a combined dataframe that will group the data by dream rather than by word, and we will have three sentiment measures: positive emotion, negative emotion and total sentiment. Let’s start by creating our two new separate dataframes for processing positive words and negative words. Here we filter by the &lt;em&gt;value&lt;/em&gt; variable, so that only rows of the dataframe with a positively valenced word get included in the dataframe &lt;em&gt;df_pos&lt;/em&gt; and similarly, only negatively valenced words are included in the &lt;em&gt;df_neg&lt;/em&gt; dataframe.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_pos &amp;lt;- df_word %&amp;gt;%
  filter(value &amp;gt; 0)
df_neg &amp;lt;- df_word %&amp;gt;%
  filter(value &amp;lt; 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we want to create composite scores per dream rather than per word. We can call this new variable &lt;em&gt;positive&lt;/em&gt; (short for sentiment per dream) in the df_pos dataframe. Because we are using a summarise function from &lt;em&gt;dplyr&lt;/em&gt; we don’t need to use a &lt;em&gt;mutate()&lt;/em&gt; to create the new variable. We are simply adding all the sent_met scores for the words in a particular dream.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_pos &amp;lt;- df_pos %&amp;gt;%
  group_by(dream_number) %&amp;gt;%
  summarise(
    positive = sum(sent_met)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can check the output of this process, while arranging by the highest value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_pos %&amp;gt;%
  arrange(desc(positive))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 314 x 2
##    dream_number positive
##           &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;
##  1          307       20
##  2          315       20
##  3          272       19
##  4          146       17
##  5          231       16
##  6          325       16
##  7          165       15
##  8          123       14
##  9          170       14
## 10          177       13
## # … with 304 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see how this has worked by filtering by dream 307, which was listed as the dream with the most positive words in it. We can filter the dream from the original df_word dataset and see which words contributed to this score. Note the list of 7 words are very upbeat! This seems like an appropriately high score for a good dream.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word %&amp;gt;%
  filter(dream_number==307)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 x 6
## # Groups:   word, value, dream_number [7]
##   word         value dream_number sample      frequency sent_met
##   &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;        &amp;lt;int&amp;gt; &amp;lt;fct&amp;gt;           &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 friendly         2          307 hall_female         1        2
## 2 wealthy          2          307 hall_female         1        2
## 3 affectionate     3          307 hall_female         1        3
## 4 beautiful        3          307 hall_female         1        3
## 5 happy            3          307 hall_female         1        3
## 6 love             3          307 hall_female         1        3
## 7 care             2          307 hall_female         2        4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then do the same process with the df_neg datafarme, aggregating the negative words across dreams.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_neg &amp;lt;- df_neg %&amp;gt;%
  group_by(dream_number) %&amp;gt;%
  summarise(
    negative = sum(sent_met)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can arrange them to see which dream contained the most negative sentiment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_neg %&amp;gt;%
  arrange(negative)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 336 x 2
##    dream_number negative
##           &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;
##  1          121      -26
##  2          191      -24
##  3          165      -22
##  4          351      -21
##  5          161      -19
##  6          173      -19
##  7          226      -19
##  8          170      -18
##  9          305      -18
## 10          397      -18
## # … with 326 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s verify this by taking a look at dream 121&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;100%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;dream&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;With a group of military prisoners, I’m standing at parade rest. Two or three officers are present. We’ve been told to direct questions to one particular inmate. After a man has asked his question, an officer loudly repeats it, the inmate answers, and the process is repeated. The men ask banal questions. The answers are equally predictable. It is all done quickly. I know that the man being questioned has committed arson. At my turn, I ask him, “Did the crime you committed put you or anyone else in danger? If so, what did you do, and why did you do it?” The other prisoners grumble. His tone is mocking as the prisoner says this is the best question, and he is asked it often. Sneering, an inmate asks me, “What about you? What did you do?” My provocative answer has put me in danger. Later these men will try to attack me. I imagine the warden will put me elsewhere in the prison. My crime was to throw hand grenades into barrels of shit&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This dream contains many negatively valenced words (“crime”, “danger”, “shit”, “mocking”). We can always look up the valence of particular words with a quick &lt;em&gt;inner_join()&lt;/em&gt; with our afinn dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;negative_words &amp;lt;- data.frame(word = c(&amp;quot;crime&amp;quot;, &amp;quot;danger&amp;quot;, &amp;quot;shit&amp;quot;, &amp;quot;mocking&amp;quot;))
afinn %&amp;gt;%
  inner_join(negative_words)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 2
##   word    value
##   &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 crime      -3
## 2 danger     -2
## 3 mocking    -2
## 4 shit       -4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;combining-sentiment-scores-into-a-single-dataframe&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Combining sentiment scores into a single dataframe&lt;/h2&gt;
&lt;p&gt;Finally, we need to bring our positive and negative sentiment scores together with our other data. Notice that each of these dataframes returns slightly less than 400 observations - 314 for the positive and 336 for the negative. This means that 86 dreams had no positive words and 64 had no negative words. If there are a few dreams with neither, we might be interested to know whether emotionally neutral dreams are special in some way, and examine them in more detail. Some studies have found high rates of emotion in dreams which would indicate that emotionally neutral dreams are more unusual (Merritt, J. M., Stickgold, R., Pace-Schott, E., Williams, J., &amp;amp; Hobson, J. A. (1994). Emotion Profiles in the Dreams of Men and Women. &lt;em&gt;Consciousness and Cognition&lt;/em&gt;, 3(1), 46–60.) &lt;a href=&#34;https://doi.org/10.1006/ccog.1994.1004&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1006/ccog.1994.1004&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To bring these two sets back together with a composite dataframe, with the dream being the unit of analysis, we need to join the two sets together, but instead of an &lt;em&gt;inner_join()&lt;/em&gt; that only retains the observations that occur in both dataframes, we want to use the &lt;em&gt;full_join()&lt;/em&gt; to retain all observations. Note the value passed to the &lt;em&gt;by=&lt;/em&gt; argument in this function requires speech marks, despite being a variable name. I am not sure why this is and it does seem a little inconsistent with the tidyverse conventions. So, if like me, you run this line and are staring at the error: “Error in common_by(by, x, y) : object ‘dream_number’ not found” - this is why!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_pos_and_neg &amp;lt;- full_join(df_pos, df_neg, by = &amp;quot;dream_number&amp;quot;)
df_pos_and_neg&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 382 x 3
##    dream_number positive negative
##           &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
##  1            2        1       NA
##  2            4        2       NA
##  3            5        3       -1
##  4            6        1       -3
##  5            8        1       -1
##  6            9        4       -3
##  7           10        3       -5
##  8           12        5       -8
##  9           13        3       -2
## 10           15        4       -3
## # … with 372 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have a few things to resolve still. Firstly we will want to create an overall sentiment value for each dream. Currently in the df_pos_and_neg dataset we only have 382 dreams, so we will also want to pull back in the missing 18 dreams and assign them a value of 0 for overall sentiment as this is why they ended up being deselected from our dataframe in the first place. Secondly, we will want to assign 0 for all the NA values in the positive and negative variables as this is what NA represents in this instance. And finally we want to pull back in the sample data from our df_word dataset.&lt;/p&gt;
&lt;p&gt;As we predicted earlier, a few dreams have been dropped from this dataset as they contained neither positive or negative emotions - the very first dream [1] is missing, so let’s have a quick look at this one.&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;100%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;dream&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;I dreamed that I was in the office of the directress or nurses in the nursing school. She is about forty-five years old. She told me the results of an I.Q. test which I had taken in a psychology class. [I really did not take this test.] The I.Q. for this test was 169. She told me that my I.Q. for the test I had taken in the nursing school was only 80. She said that if I had made a higher score in the nursing test I would have received more credit hours for the training that I received from my hospital at home. [I did take a nursing pre-entrance examination. This test determines the number of credit hours that a nurse receives for her student training when she enters the graduate school.]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Well, this dream does not appear to contain any emotions, so the sentiment analysis seems to have correctly dropped it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-our-data-back-together-again&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Putting our data back together again&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://drchristianryan.com/post/2020-02-15-sentiment-analysis-of-dreams_files/hump.jpg&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can now create a composite score of emotionality in dreams from our two variables &lt;em&gt;sent_pos&lt;/em&gt; and &lt;em&gt;sent_neg&lt;/em&gt;, by taking their absolute value and adding them together as a new variable, which we will call &lt;em&gt;sentiment&lt;/em&gt; for simplicity. To avoid losing some values when a sent_pos value exists but sent_neg is NA (or vice versa), we can include “na.rm = TRUE” at the end of the sum function.&lt;/p&gt;
&lt;p&gt;We can use our &lt;em&gt;df&lt;/em&gt; as the basis for a new full dataframe as it contains all our original data including the sample names. However, we need to put back the 400 dream_number id’s that we used in the last post, as this will be the &lt;em&gt;by=&lt;/em&gt; value for our join. Let’s name our new dataframe df_sentiment. Then we will do a full_join() with df_pos_and_neg which will pull in our two sentiment variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- df %&amp;gt;%
  mutate(dream_number = row_number()) %&amp;gt;% 
  select(-code)

df_sentiment &amp;lt;- df %&amp;gt;%
  select(-dream) %&amp;gt;% 
  full_join(df_pos_and_neg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;dream_number&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we replace our NA values with “0” for both the positive and negative variables. We can use a &lt;em&gt;mutate()&lt;/em&gt; function combined with an &lt;em&gt;if_else&lt;/em&gt; statement. The general structure of the &lt;em&gt;if_else()&lt;/em&gt; is three arguments: a conditional statement, the value to return if TRUE and the value to return if FALSE. We want the value if TRUE to simply be the same value already in the data (positive), whereas if the condition is FALSE we want to replace the NA with a “0”. The statement checks the value in each row of the &lt;em&gt;positive&lt;/em&gt; variable: if it is not an NA - !is.na() - then it keeps its value (‘positive’), else it becomes ‘0’. The same process is used on the &lt;em&gt;negative&lt;/em&gt; variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_sentiment &amp;lt;- df_sentiment %&amp;gt;%
  mutate(positive = if_else(!is.na(positive), positive, 0)) %&amp;gt;%
  mutate(negative = if_else(!is.na(negative), negative, 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can create a composite sentiment score. To let &lt;em&gt;dplyr&lt;/em&gt; know that we want to sum just the rows and not the dataframe, we use the &lt;em&gt;rowwise()&lt;/em&gt; function. We also wrap the &lt;em&gt;negative&lt;/em&gt; variable in the &lt;em&gt;abs()&lt;/em&gt; - absolute value - function so that it will ignore the sign and just return the size of sentiment to be added.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_sentiment &amp;lt;- df_sentiment %&amp;gt;%
  rowwise %&amp;gt;%
  mutate(sentiment = sum(positive, abs(negative), na.rm = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualising-the-distribution-of-sentiment-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualising the distribution of sentiment data&lt;/h2&gt;
&lt;p&gt;So how does the sentiment metric look? Let’s do a quick boxplot by sample.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_sentiment %&amp;gt;%
ggplot(aes(y = sentiment, x = sample))+
  geom_boxplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://drchristianryan.com/post/2020-02-15-sentiment-analysis-of-dreams_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that the vietnam_vet dreams appear to contain the most sentiment. In circumstances where the dataset allows us to compare the samples statistically, we could have used an ANOVA here to check whether these differences are significant. We might also want to take a look at the the single dream with the highest sentiment score. If we arrange the dataset by sentiment, in desceding order, this will give us the value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_sentiment %&amp;gt;%
  arrange(desc(sentiment))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Source: local data frame [400 x 5]
## Groups: &amp;lt;by row&amp;gt;
## 
## # A tibble: 400 x 5
##    sample      dream_number positive negative sentiment
##    &amp;lt;chr&amp;gt;              &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
##  1 vietnam_vet          165       15      -22        37
##  2 vietnam_vet          170       14      -18        32
##  3 hall_male            226       11      -19        30
##  4 vietnam_vet          113       11      -17        28
##  5 vietnam_vet          121        2      -26        28
##  6 hall_male            272       19       -9        28
##  7 hall_female          315       20       -6        26
##  8 vietnam_vet          191        0      -24        24
##  9 hall_male            279        8      -16        24
## 10 vietnam_vet          106        9      -14        23
## # … with 390 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Dream number 165 has the highest sentiment score, and we can see that it contains both negative and positive sentiment. This is the benefit of our split-apply-combine approach. You can imagine that if we had simply combined our values the positive 15 and negative 22 would have resulted in total score of only -7. Let’s take a look at this dream, and see what sentiment it contains.&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;100%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;dream&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;I’m young. While riding in a railroad boxcar with other people I meet a girl, a plain looking tomboy whose personality makes her alluring. The conductor explains that the center of the wooden floor is made of glass, it will not support the heater I’ve brought. No matter. I will lend it to the man who has asked to use it. One, two or three weeks at a time he can take it wherever he likes. We arrive at a summer village, which the girl belongs to. Several times, as we walk down a dirt path bordered by chicken wire, I ask if the area is safe. The girl assures me it is, and points out several dark green wooden rowboats tied up to the shore. With these, we can take lessons in a specific rowboat technique. When we sleep next to each other, we talk, and several times I choke up. It’s the war. I’m thinking how awful things can get, but I do not tell her that’s what bothers me. When we arrive at a busy amusement park there are people crowded by entrance. A woman who looks like a woman who resembles Chris D elbows the girl several times. In the slow moving melee I lose sight of the girl, who has entered the amusement park. We haven’t made emergency plans; how will I know where to look for her, or she for me? It’s as if everything is lost. However I decide to stay put and wait for her. I’m so happy when she returns. I really like this girl. She is my friend. I point out the woman who elbowed her. She seems an angry woman. Angry at life&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;And as we expected this does is a dream with many of positive words (‘like’, ‘amusement’, ‘safe’) and negative ones (‘lose’, ‘angry’, ‘war’).&lt;/p&gt;
&lt;p&gt;We could also compare the four samples on positive and negative emotions separately.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_sentiment %&amp;gt;%
ggplot(aes(y = positive, x = sample))+
  geom_boxplot()+
  labs(title = &amp;quot;Positive emotion by sample&amp;quot;)+
  theme(plot.title = element_text(hjust = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://drchristianryan.com/post/2020-02-15-sentiment-analysis-of-dreams_files/figure-html/unnamed-chunk-33-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For the negative emotion, I have taken the absolute value with &lt;em&gt;abs()&lt;/em&gt; function so that that it plots in the same direction as positive emotion.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_sentiment %&amp;gt;%
ggplot(aes(y = abs(negative), x = sample))+
  geom_boxplot()+
  labs(title = &amp;quot;Negative emotion by sample&amp;quot;)+
  ylab(&amp;quot;negative&amp;quot;)+
  theme(plot.title = element_text(hjust = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://drchristianryan.com/post/2020-02-15-sentiment-analysis-of-dreams_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scatterplot-of-positive-and-negative-sentiment-by-dream&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scatterplot of positive and negative sentiment by dream&lt;/h2&gt;
&lt;p&gt;We can plot the two sentiment vectors against one another, to see if any pattern emerges between the two sentiment scores. The alpha value (transparency) is set to .7, as many of the points overlap and increasing the transparency can aid the readability of the graph. We also add a bit of jitter to avoid overplotting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_sentiment %&amp;gt;%
  ggplot(aes(abs(negative), positive, colour = sample))+
  scale_color_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;skyblue&amp;quot;, &amp;quot;orange&amp;quot;, &amp;quot;darkgreen&amp;quot;))+
  xlab(&amp;quot;Negative sentiment&amp;quot;)+
  ylab(&amp;quot;Positive sentiment&amp;quot;)+
  geom_jitter(alpha = .5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://drchristianryan.com/post/2020-02-15-sentiment-analysis-of-dreams_files/figure-html/unnamed-chunk-35-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is no clear relationship between the positive and negative emotion. Clustering close to either axis would have implied that many dreams contain high amounts of positive or negative emotion, but not both. Whereas we see quite a spread across the graph, which shows that dreams can vary across positive and negative emotions. However, uni-direction emotional dreams do occur as well. If we look up the y axis we can see a number of blue and green dots from y = 10 to = y 22 where the value for x is probably 0 (after taking account of the error introduced in the jitter). These are dreams with only positive sentiment. The same uniformity can be seen in some of the dreams represented by green dots on the x axis from x = 12 to x = 23.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;word-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Word count&lt;/h2&gt;
&lt;p&gt;We have not controlled for word count in our analysis of sentiment. It is possible longer dreams are more likely to have sentiment words in them. We could pull in the word count data from one of the previous dataframes to check this. Here we unnest the tokens from the original dataframe (df), but we don’t remove the stopwords. We then group_by the dream_number and calculate a count variable of the number of words in each dream.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_freq &amp;lt;- df %&amp;gt;%
  unnest_tokens(word, dream) %&amp;gt;%
  group_by(dream_number)%&amp;gt;%
  summarise(count = n())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can paste this new variable back in to our df_sentiment database with a left_join.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_sentiment &amp;lt;- df_sentiment %&amp;gt;%
  left_join(df_freq, by = &amp;quot;dream_number&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can run a quick correlation on &lt;em&gt;sentiment&lt;/em&gt; and &lt;em&gt;count&lt;/em&gt; to see if there is a relationship.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;with(df_sentiment, cor.test(sentiment, count))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Pearson&amp;#39;s product-moment correlation
## 
## data:  sentiment and count
## t = 9.287, df = 398, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.3379632 0.4994153
## sample estimates:
##       cor 
## 0.4220297&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is a positive correlation, so it would be sensible to convert out sentiment metric to a &lt;em&gt;sentiment per word&lt;/em&gt; scale. We can call this “sent_prop” - short for ‘sentiment proportion’. We will also multiply by 100 to express sent_prop as a percentage of words used in each dream.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_sentiment &amp;lt;- df_sentiment %&amp;gt;%
  mutate(sent_prop = (sentiment/count)*100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see if this makes any difference to the distribution of sentiment across the 4 samples of dreams.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_sentiment %&amp;gt;%
  ggplot(aes(x = sample, y = sent_prop))+
  geom_boxplot()+
  ylab(&amp;quot;Percentage of sentiment words per dream&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://drchristianryan.com/post/2020-02-15-sentiment-analysis-of-dreams_files/figure-html/unnamed-chunk-40-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This has reduced some of the differences between the samples. We could check if the earlier difference in sentiment words between the samples was due to variations in word count by running a boxplot on our &lt;em&gt;count&lt;/em&gt; variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_sentiment %&amp;gt;%
  ggplot(aes(x = sample, y = count))+
  geom_boxplot()+
  ylab(&amp;quot;Number of words per dream&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://drchristianryan.com/post/2020-02-15-sentiment-analysis-of-dreams_files/figure-html/unnamed-chunk-41-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This does appear to be the case. This shows how important controlling for word count could be in comparing texts. However, the &lt;em&gt;sentiment per word&lt;/em&gt; needs to be treated with some caution. For instance, we still might want to know why some people express both more words and more emotion, rather than less words and less emotion. Looking at the last boxplot for instance, could make you wonder, why do the college women appear to use less words to describe their dreams that the hall females? However, investigating questions such as these is beyond the scope of this post.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2.3 - Using tidytext to compare samples of dreams</title>
      <link>https://drchristianryan.com/2020/01/27/using-tidytext-to-compare-samples-of-dreams/</link>
      <pubDate>Mon, 27 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://drchristianryan.com/2020/01/27/using-tidytext-to-compare-samples-of-dreams/</guid>
      <description>


&lt;p&gt;This is the third post in the series exploring text analytics with data from the dreambank.com. In the first post ‘Pulling text data from the internet’, I demonstrated how to use the &lt;strong&gt;rvest&lt;/strong&gt; package to pull text data from the dreambank website. In the second post ‘Manipulating text data from dreams’ we saw how to turn the dream texts into a tidy format by unnesting the word tokens in each dream and running counts on the word frequencies. In this third post of the series, I am going to demonstrate some ways of comparing texts from Julia Silge and David Robinson’s book &lt;strong&gt;Text Mining with R - A tidy approach&lt;/strong&gt; using the dream data set to illustrate the ideas, while also unpacking some of the steps a little further than in their book.&lt;/p&gt;
&lt;p&gt;We will load our packages again and pull in the same data we analysed last time (see the previous post for details).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tidytext)
library(stringr)
library(car)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The processing steps we used last time on the data was to add a dream number, unnest the tokens, remove stopwords, and then filter out underscores and digits.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word &amp;lt;- df %&amp;gt;%
  mutate(dream_number = row_number()) %&amp;gt;%
  unnest_tokens(word, dream) %&amp;gt;%
  anti_join(stop_words) %&amp;gt;%
  filter(str_detect(word, pattern = &amp;quot;_&amp;quot;, negate = TRUE)) %&amp;gt;%
  filter(str_detect(word, pattern = &amp;#39;[\\d]&amp;#39;, negate = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we calculated word frequencies as proportions. This time we will store this as a new dataframe called df_proportion. Notice we have to remove the temporary ‘n’ variable. I am not sure why this is, but if you don’t deselect it, it seems to mess with the spread() function, creating multiple rows for the same word.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_proportion &amp;lt;- df_word %&amp;gt;%
  group_by(sample, word) %&amp;gt;%
  summarise(n = n()) %&amp;gt;%
  mutate(percent = (n / sum(n))*100) %&amp;gt;%
  mutate(percent = round(percent, 2)) %&amp;gt;%
  select(-n) %&amp;gt;%
  arrange(desc(percent)) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;comparing-word-frequencies-across-samples&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Comparing word frequencies across samples&lt;/h1&gt;
&lt;p&gt;We might want to compare the frequencies across samples. We can use a technique that Juile Silge and David Robinson used to compare word frequencies across authors. This is a clever trick in which they use the &lt;em&gt;spread()&lt;/em&gt; and &lt;em&gt;gather()&lt;/em&gt; functions. Spread gives each sample their own column and makes the value proportion. Rather than do the spread and gather in one code chuck as in the book, I will do them as two separate stages to illustrate the process in more detail.&lt;/p&gt;
&lt;p&gt;So let’s spread the data first. Think of this as taking the column of percentages/proportions and moving it into four separate columns: one for each sample. We are passing two arguments to the &lt;em&gt;spread()&lt;/em&gt; function, the key (which contains the names of the items to form new columns) which is our ‘sample’ variable, and the value (what will the cells in each of these columns be filled with), which is the variable ‘percent’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_spread &amp;lt;- df_proportion %&amp;gt;%
  spread(sample, percent)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could look at this new dataframe directly by clicking on the icon in the Global Environment, but you will notice lots of NA values. This is because each dream sample contains words that are unique to that sample and in a way these are the least informative - we can only make a binary comparison if one sample contains the word and the other does not. So we really want to see the variance in proportions for words that occur in more than one sample. To take a better look at these examples we can run our new dataframe called df_spread through a !is.na (is not NA) filter and then call the &lt;em&gt;some()&lt;/em&gt; function from the &lt;strong&gt;car&lt;/strong&gt; package. This is simply filtering out the rows in which the NA value occurs for the proportion of a word in any of our four samples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_spread %&amp;gt;%
  filter(!is.na(college_women) &amp;amp; !is.na(hall_female) &amp;amp; !is.na(hall_male) &amp;amp; !is.na(vietnam_vet)) %&amp;gt;%
  some()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 5
##    word      college_women hall_female hall_male vietnam_vet
##    &amp;lt;chr&amp;gt;             &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
##  1 beautiful          0.12       0.290      0.02        0.14
##  2 carried            0.09       0.05       0.02        0.02
##  3 crowd              0.06       0.1        0.09        0.03
##  4 familiar           0.03       0.05       0.05        0.03
##  5 fire               0.06       0.05       0.02        0.08
##  6 hurt               0.03       0.07       0.09        0.08
##  7 lot                0.06       0.17       0.14        0.08
##  8 sidewalk           0.03       0.1        0.02        0.03
##  9 staying            0.06       0.1        0.02        0.05
## 10 taking             0.09       0.05       0.09        0.09&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then the next step is to decide which sample is going to be our reference sample. This will have its column of frequencies replicated and stacked so that each of the other samples can be compared with it. When we &lt;em&gt;gather()&lt;/em&gt;, we only include the samples to compare with the reference and not the reference itself. For simplicity, we can use the first column in our df (college_women) as our reference group and compare the other three samples to this. We might predict at this point that the hall_women will have the most similar dreams and then the hall_men, with the vietnam_vet being the most different. We can check this prediction later on when we run some correlation coefficients.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_gather &amp;lt;- df_spread %&amp;gt;%
  gather(sample, proportion, hall_female:vietnam_vet)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can take a quick look at the structure of the output, but we will filter out the NA values first, as we did for the df_spread dataset, as many are created for words that appear in one sample of dreams but not the other. Notice we do the same not-NA process (!is.na) on both the college_women data and the “proportion” variable. Remember that the college_women are our reference sample, so the ‘sample’ in this case tells us who provide the data in the proportion column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_gather %&amp;gt;%
  filter(!is.na(college_women) &amp;amp; !is.na(proportion))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,950 x 4
##    word     college_women sample      proportion
##    &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;
##  1 accident          0.03 hall_female       0.02
##  2 act               0.06 hall_female       0.05
##  3 afraid            0.24 hall_female       0.1 
##  4 age               0.61 hall_female       0.27
##  5 aged              0.09 hall_female       0.02
##  6 ages              0.03 hall_female       0.05
##  7 ago               0.12 hall_female       0.15
##  8 ahead             0.06 hall_female       0.02
##  9 air               0.15 hall_female       0.02
## 10 alarm             0.09 hall_female       0.02
## # … with 1,940 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we can graph this data to inspect it more effectively. We use the df_gather as our data and set the aesthetics for x and y to proportion (which are our three samples) and college_women - which is our reference sample. As the college_women data will be used in all three graphs, it makes sense to assign it to the y axis, so we can scan down all three graphs vertically to make comparisons. There is another clever trick here with the colour variable. We will set it to the absolute value - &lt;em&gt;abs()&lt;/em&gt; - of the difference between the college_women value and the other sample (proportion) value. This means that the darker the colour of the dot, the stronger the difference between the two proportion values. This allows us to use both shade (dark to light) and position away from the diagonal line as a measure of difference between samples. The paler the text the more similar the proportion use of the word in both samples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(scales)

ggplot(df_gather, aes(x = proportion, y = college_women,
                 colour = abs(college_women - proportion)))+
  geom_abline(colour = &amp;quot;gray40&amp;quot;, lty = 2)+
  geom_jitter(alpha = 0.2, size = 2, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 2)+
  scale_x_log10(labels = percent_format())+
  scale_y_log10(labels = percent_format())+
  scale_color_gradient(limits = c(0, 0.22),
                       low = &amp;quot;grey&amp;quot;, high = &amp;quot;black&amp;quot;)+
  facet_wrap(~sample, ncol = 1)+
  theme(legend.position = &amp;quot;none&amp;quot;)+
  labs(y = &amp;quot;college_women&amp;quot;, x = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://drchristianryan.com/post/2020-01-27-using-tidytext-to-compare-samples-of-dreams_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The words on or close to the line represent words that occurred at similar frequencies in both samples. As an example, we can see the word ‘remember’ occurred at high frequency in the first graph, in both the college_women and hall_female samples, as it is both high up and far to the right on the graph - but we can also observe that the closeness to the line suggests a similar frequency between the samples. In contrast, we can see on the same graph that the word “aunt” is quite far to the left of the line, indicating that it occurs more frequently in the college_women than the hall_women. This is also the case in the graph of college_women against hall_men, whereas this is not obvious in the final graph. It is possible that the word doesn’t occur at all in the vietnam_vet dream - therefore it would have been removed by graph function call. We can check the values for ‘aunt’ with a quick call of the df_spread data.frame (this is why it can be useful to keep both versions, rather than overwriting the data when we used the gather function).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_spread %&amp;gt;%
  filter(word == &amp;quot;aunt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 5
##   word  college_women hall_female hall_male vietnam_vet
##   &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 aunt           0.43        0.02      0.02          NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we predicted, the lack of ‘aunt’ in the final graph was not due to a similar frequency between the samples, but rather the complete absence of the word in the vietnam_vet dreams. We can also see the size of difference between the frequency of the word in the college_women’s dreams and the other three samples is very large. Graphing the words in this way can give you a strong sense of these differences between the texts.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-coefficient&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Correlation coefficient&lt;/h1&gt;
&lt;p&gt;We can use the base R function cor.test() to measure the degree of similarity between the propotions of words used in each sample. One approach is to prepare a new dataframe to carry out this test. Here we create a dataframe called df_cwhf(college_women and hall_female) and filter for just those rows that represent data for college women and hall_females. We can then run the &lt;em&gt;cor.test()&lt;/em&gt; on this dataframe, by passing the variables ‘college_women’ and ‘proportion’ - the latter being just the proportion for the hall_female (because of the filter we applied).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_cwhf &amp;lt;- df_gather %&amp;gt;%
  filter(sample == &amp;quot;hall_female&amp;quot;)
cor.test(df_cwhf$college_women, df_cwhf$proportion)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Pearson&amp;#39;s product-moment correlation
## 
## data:  df_cwhf$college_women and df_cwhf$proportion
## t = 34.676, df = 643, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.7785169 0.8325231
## sample estimates:
##       cor 
## 0.8072027&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, we could use the syntax in &lt;strong&gt;Text mining with R&lt;/strong&gt;, which subsets the data on the fly. We explicitly give a data argument, which is the df_gather dataframe, subsetted with &lt;em&gt;samples == “hall_female”&lt;/em&gt;. We add a comma after this in the square brackets &lt;strong&gt;[ ]&lt;/strong&gt;, with no other argument, as we want to retain all the columns. Finally we provide our x and y values to be correlated in the formula format &lt;strong&gt;~ x + y&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor.test(data = df_gather[df_gather$sample == &amp;quot;hall_female&amp;quot;, ], ~ proportion + college_women)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Pearson&amp;#39;s product-moment correlation
## 
## data:  proportion and college_women
## t = 34.676, df = 643, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.7785169 0.8325231
## sample estimates:
##       cor 
## 0.8072027&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s use this one-line technique to run the other two comparisons.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor.test(data = df_gather[df_gather$sample == &amp;quot;hall_male&amp;quot;, ], ~ proportion + college_women)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Pearson&amp;#39;s product-moment correlation
## 
## data:  proportion and college_women
## t = 28.24, df = 648, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.7062132 0.7753868
## sample estimates:
##       cor 
## 0.7427756&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor.test(data = df_gather[df_gather$sample == &amp;quot;vietnam_vet&amp;quot;, ], ~ proportion + college_women)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Pearson&amp;#39;s product-moment correlation
## 
## data:  proportion and college_women
## t = 12.03, df = 653, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.3610922 0.4866471
## sample estimates:
##      cor 
## 0.425918&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This reveals that all three samples correlate highly with the college_women, but the vietnam_vet dreams had a much lower correlation (.43) than the hall_female (.81) or the hall_male (.74). We could go on to use term frequency–inverse document frequency (tf-idf) to make a more complex comparison between the samples. In the next post, we will apply some of the sentiment analysis ideas from the book to the dream data. Let’s save our dataset df_word for next time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;save(df_word, file = &amp;quot;df_word.Rdata&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2.2 - Manipulating text data from dreams</title>
      <link>https://drchristianryan.com/2020/01/14/manipulating-text-data-from-dreams/</link>
      <pubDate>Tue, 14 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://drchristianryan.com/2020/01/14/manipulating-text-data-from-dreams/</guid>
      <description>


&lt;p&gt;In the previous post on ‘pulling text data from the internet’, I experimented with pulling out the dream text from a sample of dreams from the website “DreamBank” at: &lt;a href=&#34;http://www.dreambank.net/random_sample.cgi&#34; class=&#34;uri&#34;&gt;http://www.dreambank.net/random_sample.cgi&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this follow-up post, I will demonstrate some of the methods presented in Julia Silge and David Robinson’s book ‘Text Mining with R’ for processing text data, as applied to 400 dreams sampled from 4 collections in the dreambank. I used the methods described in the last post to pull out a random sample of 100 dreams from each of the following 4 groups:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;college_women (this was the sample used last time)&lt;/li&gt;
&lt;li&gt;hall_female&lt;/li&gt;
&lt;li&gt;hall_male&lt;/li&gt;
&lt;li&gt;vietnam_vet&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first set of dreams were recorded by college women by Calvin Hall from undergraduates in a course on personality at Western Reserve University in 1947 and 1948.&lt;/p&gt;
&lt;p&gt;The second and third samples are also dreams collected by Calvin Hall and Robert L. Van de Castle, on which they based female and male norms in their book &lt;em&gt;The Content Analysis of Dreams&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The sample listed as vietnam_vet are from the dreams of an American veteran of the Vietnam war, who suffered PTSD. The website has over 400 of his dreams which he donated from records he kept not long after returning from Vietnam.&lt;/p&gt;
&lt;p&gt;Let’s begin by loading the three packages we are likely to use.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tidytext)
library(stringr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to follow along with this post, the dataset I am about to load is “dream_df.csv”, which can be found on my github page: &lt;a href=&#34;https://github.com/Christian-Ryan/netsite/tree/master/public/post&#34; class=&#34;uri&#34;&gt;https://github.com/Christian-Ryan/netsite/tree/master/public/post&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- read_csv(&amp;quot;dreams_df.csv&amp;quot;)
df &amp;lt;- df[,2:3]
df$sample &amp;lt;- as.factor(df$sample)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After sampling the four dream sets, using the techniques described in the last post, we now have a dataframe called df with two variables - &lt;em&gt;sample&lt;/em&gt; and &lt;em&gt;dream&lt;/em&gt;. We will use our custom_view() function we created last time to display snippets of dreams neatly formatted. We can also use the &lt;em&gt;some()&lt;/em&gt; function from the car package to take a quick look at a selection of dreams across the dataframe. The &lt;em&gt;some()&lt;/em&gt; function is very like &lt;em&gt;head()&lt;/em&gt; and &lt;em&gt;tail()&lt;/em&gt;, but has the advantage of returning a selection across the dataset, which allows us to see examples from each of the samples simultaneously.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;custom_view &amp;lt;- function(x) data.frame(lapply(x, substr, 1, 56))
car::some(df) %&amp;gt;%
  custom_view()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           sample                                                    dream
## 1  college_women I dreamed I was driving down town to get a book I had le
## 2  college_women I was walking up my street when I heard the wild barking
## 3  college_women I was with my sister, Elizabeth. We went on a train ride
## 4    hall_female I dreamt that a friend of mine who graduated last year c
## 5    hall_female I dreamed that Dean Dolan was explaining the constructio
## 6    hall_female I went into the reserve bookstore to buy some green-line
## 7    hall_female I was driving home from the show alone, and I was at Ric
## 8    hall_female My parents took in a male boarder, and we were attracted
## 9    hall_female This dream took place entirely in one room and instead o
## 10   hall_female I dreamt that a fellow I know came to the house and we w&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://drchristianryan.com/post/2020-01-14-manipulating-text-data-from-dreams_files/tidytext.png&#34; style=&#34;width:35.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Julia Silge and David Robinson’s book &lt;strong&gt;Text Mining with R - A tidy approach&lt;/strong&gt; sets off at a cracking pace, at least for relatively newbies to R such as myself. They assumes a degree of familiarity with tidyverse concepts and when they introduce concepts such as tidytext format, they can sometimes address three or four steps in one example. I will unpack some of these as individual steps to illustrate what is going on, while using our dream data as the material for processing.&lt;/p&gt;
&lt;p&gt;At the moment our df only contains the sample name (a categorical variable with four values) and the text of the dream. It might be helpful to index the dreams before we tokenise the text in them. So let’s introduce a new variable that we will call dream_number. This will index each dream between 1 - 400 in the dataframe.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;-  df %&amp;gt;%
  mutate(dream_number = row_number())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have the dream_number variable added, we can unnest the tokens (split the text variable into individual words). The syntax for the unnest_tokens() function is to pipe in the dataframe (df), then supply the name of the variable to be created (word), followed by the variable containing the text we are going to tokenise - in this case “dream”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word &amp;lt;- df %&amp;gt;%
  unnest_tokens(word, dream)
head(df_word)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   sample        dream_number word   
##   &amp;lt;fct&amp;gt;                &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;  
## 1 college_women            1 i      
## 2 college_women            1 dreamed
## 3 college_women            1 that   
## 4 college_women            1 i      
## 5 college_women            1 was    
## 6 college_women            1 in&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See that the &lt;em&gt;word&lt;/em&gt; variable has replaced our &lt;em&gt;dream&lt;/em&gt; variable and now each word is on a separate row - this is the tidytext format. &lt;em&gt;unnest_tokens()&lt;/em&gt; has kept the variables &lt;em&gt;sample&lt;/em&gt; and &lt;em&gt;dream_number&lt;/em&gt; - it only transforms the input variable (dream) into the output variable (word). Notice also that the function has transformed into lower-case all the words in the &lt;em&gt;word&lt;/em&gt; variable.&lt;/p&gt;
&lt;div id=&#34;tokenisation-and-n-grams&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Tokenisation and N-Grams&lt;/h1&gt;
&lt;div id=&#34;section&#34; class=&#34;section level6&#34;&gt;
&lt;h6&gt;&lt;/h6&gt;
&lt;p&gt;It should be noted that when we use unnest_tokens() we are using a range of default values. We could have specified something other than single words in our output. The default value of the token argument is ‘word’. We can change this to ‘ngram’ and use an ‘n=’ to specify how many words should be kept as a group. Let us try a quick run with 3-word tokens instead of single words to demonstrate this behaviour.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_trigrams &amp;lt;- df %&amp;gt;%
  unnest_tokens(trigrams, dream, token = &amp;quot;ngrams&amp;quot;, n = 3)
head(df_trigrams)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   sample        dream_number trigrams      
##   &amp;lt;fct&amp;gt;                &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;         
## 1 college_women            1 i dreamed that
## 2 college_women            1 dreamed that i
## 3 college_women            1 that i was    
## 4 college_women            1 i was in      
## 5 college_women            1 was in the    
## 6 college_women            1 in the office&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So here we have set our output variable to ‘trigrams’ and specified the token argument to be equal to ‘ngrams’, and we have saved this as a new dataframe called ‘df_trigrams’. That gives us a better sense of the nature of the text. We can also run a count on this after grouping by sample.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_trigrams %&amp;gt;%
  group_by(sample) %&amp;gt;%
  count(trigrams, sort = TRUE) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 50,935 x 3
##    sample        trigrams              n
##    &amp;lt;fct&amp;gt;         &amp;lt;chr&amp;gt;             &amp;lt;int&amp;gt;
##  1 vietnam_vet   i tell him           33
##  2 hall_female   i was in             29
##  3 college_women i was in             25
##  4 vietnam_vet   the scene changes    23
##  5 college_women and i was            22
##  6 hall_female   seemed to be         19
##  7 college_women that i was           18
##  8 hall_female   that i was           17
##  9 hall_male     seemed to be         17
## 10 hall_female   and i was            16
## # … with 50,925 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we can see that in the Vietnam veteran dream sample, the most common three word phrase was “I tell him”, whereas for the Hall Female and College Women the most common phrase was “I was in”. Using ngrams (units larger than one word), can be useful in exploring most frequently occurring phrases. It is notable that the phrase for the Vietnam vet was in the present tense, giving a sense of the immediacy and immersion of the dream experience, whereas those most frequent phrases of the other samples are in the past tense.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;single-words-bag-of-words-approach&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Single words (Bag of words approach)&lt;/h1&gt;
&lt;p&gt;We have not removed stop-words yet as this would undermine our exploration of ngrams. But this is the next step for our df_word dataset. The &lt;em&gt;anti_join()&lt;/em&gt; function, takes two dataframes and keeps only those words that don’t occur in both dataframes. So this forms a convenient and easy way to filter out unwanted stopwords.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word &amp;lt;- df_word %&amp;gt;%
  anti_join(stop_words)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can count the words and sort them into descending order.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word %&amp;gt;%
  count(word, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5,331 x 2
##    word         n
##    &amp;lt;chr&amp;gt;    &amp;lt;int&amp;gt;
##  1 house      133
##  2 dream      132
##  3 remember   125
##  4 car        118
##  5 people     110
##  6 girl       108
##  7 friend     101
##  8 time        95
##  9 woman       93
## 10 mother      85
## # … with 5,321 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But before we create some plots of these words, we should check for any anomalies in the &lt;em&gt;word&lt;/em&gt; variable of df_word. The sorted count is likely to give back expected results (high frequency genuine words). But there can be other text elements that we may want to filter out. This will become obvious if we count, but don’t sort.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word %&amp;gt;%
  count(word)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5,331 x 2
##    word       n
##    &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;
##  1 ___        1
##  2 ______     1
##  3 00         2
##  4 1          4
##  5 1,500      1
##  6 10        13
##  7 100        3
##  8 105        1
##  9 107th      1
## 10 109        1
## # … with 5,321 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;em&gt;word&lt;/em&gt; variable contains some text elements that we would not regard as words. Let’s check where the underscores came from. To do this we must go back to our original (untokenised) dataset &lt;em&gt;df&lt;/em&gt;, as we want to see the underscores in the context of the dream. We can use the &lt;em&gt;str_which()&lt;/em&gt; function to identify which dreams contain underscores, matched to the pattern &lt;code&gt;&#39;___&#39;&lt;/code&gt;. Then we can use this as an index on the df$dream variable, so that it just returns the context of the dreams with underscores. As there are three dreams with underscores, we will store this sequence of dreams and then take a look at the first one.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;underscores &amp;lt;- df$dream[str_which(df$dream, pattern = &amp;quot;___&amp;quot;)]
underscores[1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;I dreamed about a young married couple whom I have known for a long time. They came to see us at our home. Although the home was ours, it resembled my Uncle&amp;#39;s home in C___ and yet the dream seemed to take place in C ___.. They drove up in a Model A Ford &amp;amp; parked it in the front yard. We were in the living room talking when another Model A Ford drove up &amp;amp; in it were my sister &amp;amp; a friend of mine. I went out in the front yard, got in this couple&amp;#39;s car, and started to talk to my sister. D___ my sister, asked me if I wanted to go to a play with J. She said that she and her husband weren&amp;#39;t going. I realized that I would have to go with him alone, so I refused. Then they drove away and the wife came out in the yard. She seemed perturbed at my getting into their car, so she got into the car and backed it away. The car then suddenly changed into an old-fashioned bicycle. It was at this time that I felt antagonistic towards this couple.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the pattern here seems to be that underscores are used to disguise the identity of named people in the dreams. We can choose to filter these out as they are not relevant to our analysis. But before we do this filtering, let’s also consider the numbers in the &lt;em&gt;word&lt;/em&gt; variable column - again in a bag-of-words approach one could argue that these are not words and so are irrelevant. We want to create a pattern that identifies both digits and underscores, and then use a function to transform our &lt;em&gt;word&lt;/em&gt; variable in the df_word dataframe.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-pattern-to-remove-numbers-and-underscores&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Create pattern to remove numbers and underscores&lt;/h1&gt;
&lt;p&gt;We can use the function str_subset() to identify the elements of the &lt;em&gt;word&lt;/em&gt; variable that we wish to remove. Let’s create a pattern that deals initially with the underscores and try &lt;em&gt;str_subset()&lt;/em&gt; with it. The ‘+’ is not strictly necessary here, but it illustrates that we can identify at least one underscore by this combination.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_subset(df_word$word, pattern = &amp;#39;_+&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;n__&amp;quot;    &amp;quot;y__&amp;quot;    &amp;quot;c___&amp;quot;   &amp;quot;___&amp;quot;    &amp;quot;d___&amp;quot;   &amp;quot;h___&amp;quot;   &amp;quot;a___&amp;quot;   &amp;quot;a___&amp;quot;  
##  [9] &amp;quot;h__&amp;quot;    &amp;quot;______&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This has found ten instances of the underscore in the word variable. Now we want to find all the digits. We could use the regex shorthand &lt;em&gt;[\d]&lt;/em&gt; or &lt;em&gt;[:digit:]&lt;/em&gt;. Let’s use the latter first with &lt;em&gt;str_subset&lt;/em&gt; to check it works.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_subset(df_word$word, pattern = &amp;#39;[:digit:]&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [1] &amp;quot;169&amp;quot;    &amp;quot;80&amp;quot;     &amp;quot;90&amp;quot;     &amp;quot;30&amp;quot;     &amp;quot;60&amp;quot;     &amp;quot;40&amp;quot;     &amp;quot;45&amp;quot;     &amp;quot;4&amp;quot;     
##   [9] &amp;quot;20&amp;quot;     &amp;quot;4&amp;quot;      &amp;quot;5&amp;quot;      &amp;quot;2&amp;quot;      &amp;quot;34&amp;quot;     &amp;quot;34&amp;quot;     &amp;quot;309&amp;quot;    &amp;quot;219&amp;quot;   
##  [17] &amp;quot;6&amp;quot;      &amp;quot;5.00&amp;quot;   &amp;quot;8&amp;quot;      &amp;quot;5&amp;quot;      &amp;quot;45&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;45&amp;quot;     &amp;quot;22&amp;quot;    
##  [25] &amp;quot;50&amp;quot;     &amp;quot;23&amp;quot;     &amp;quot;45&amp;quot;     &amp;quot;22&amp;quot;     &amp;quot;30&amp;quot;     &amp;quot;45&amp;quot;     &amp;quot;22&amp;quot;     &amp;quot;11&amp;quot;    
##  [33] &amp;quot;8&amp;quot;      &amp;quot;8&amp;quot;      &amp;quot;12&amp;quot;     &amp;quot;3rd&amp;quot;    &amp;quot;26&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;22&amp;quot;    
##  [41] &amp;quot;60&amp;quot;     &amp;quot;70&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;25&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;27&amp;quot;     &amp;quot;23&amp;quot;     &amp;quot;52&amp;quot;    
##  [49] &amp;quot;23&amp;quot;     &amp;quot;7&amp;quot;      &amp;quot;30&amp;quot;     &amp;quot;2nd&amp;quot;    &amp;quot;2nd&amp;quot;    &amp;quot;7&amp;quot;      &amp;quot;10&amp;quot;     &amp;quot;10&amp;quot;    
##  [57] &amp;quot;2&amp;quot;      &amp;quot;1&amp;quot;      &amp;quot;1&amp;quot;      &amp;quot;2&amp;quot;      &amp;quot;999&amp;quot;    &amp;quot;e1&amp;quot;     &amp;quot;10&amp;quot;     &amp;quot;2&amp;quot;     
##  [65] &amp;quot;4&amp;quot;      &amp;quot;2&amp;quot;      &amp;quot;2&amp;quot;      &amp;quot;2&amp;quot;      &amp;quot;25&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;5&amp;quot;      &amp;quot;22&amp;quot;    
##  [73] &amp;quot;20&amp;quot;     &amp;quot;8&amp;quot;      &amp;quot;30&amp;quot;     &amp;quot;6&amp;quot;      &amp;quot;8&amp;quot;      &amp;quot;30&amp;quot;     &amp;quot;8&amp;quot;      &amp;quot;30&amp;quot;    
##  [81] &amp;quot;50&amp;quot;     &amp;quot;4&amp;quot;      &amp;quot;35&amp;quot;     &amp;quot;4&amp;quot;      &amp;quot;00&amp;quot;     &amp;quot;40&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;5&amp;quot;     
##  [89] &amp;quot;10&amp;quot;     &amp;quot;2&amp;quot;      &amp;quot;80&amp;quot;     &amp;quot;45&amp;quot;     &amp;quot;48&amp;quot;     &amp;quot;55&amp;quot;     &amp;quot;22&amp;quot;     &amp;quot;40&amp;quot;    
##  [97] &amp;quot;1992&amp;quot;   &amp;quot;200&amp;quot;    &amp;quot;300&amp;quot;    &amp;quot;100&amp;quot;    &amp;quot;20s&amp;quot;    &amp;quot;30s&amp;quot;    &amp;quot;1950s&amp;quot;  &amp;quot;2001&amp;quot;  
## [105] &amp;quot;2012&amp;quot;   &amp;quot;10&amp;quot;     &amp;quot;12&amp;quot;     &amp;quot;1990s&amp;quot;  &amp;quot;50s&amp;quot;    &amp;quot;1972&amp;quot;   &amp;quot;1950s&amp;quot;  &amp;quot;800&amp;quot;   
## [113] &amp;quot;45&amp;quot;     &amp;quot;60s&amp;quot;    &amp;quot;1970&amp;quot;   &amp;quot;45&amp;quot;     &amp;quot;1960s&amp;quot;  &amp;quot;105&amp;quot;    &amp;quot;1st&amp;quot;    &amp;quot;109&amp;quot;   
## [121] &amp;quot;110&amp;quot;    &amp;quot;116&amp;quot;    &amp;quot;121&amp;quot;    &amp;quot;122&amp;quot;    &amp;quot;2001&amp;quot;   &amp;quot;2012&amp;quot;   &amp;quot;138&amp;quot;    &amp;quot;139&amp;quot;   
## [129] &amp;quot;152&amp;quot;    &amp;quot;m16&amp;quot;    &amp;quot;m60&amp;quot;    &amp;quot;59&amp;quot;     &amp;quot;2001&amp;quot;   &amp;quot;2012&amp;quot;   &amp;quot;39&amp;quot;     &amp;quot;244&amp;quot;   
## [137] &amp;quot;1200&amp;quot;   &amp;quot;207&amp;quot;    &amp;quot;208&amp;quot;    &amp;quot;209&amp;quot;    &amp;quot;211&amp;quot;    &amp;quot;214&amp;quot;    &amp;quot;215&amp;quot;    &amp;quot;216&amp;quot;   
## [145] &amp;quot;800&amp;quot;    &amp;quot;411&amp;quot;    &amp;quot;42nd&amp;quot;   &amp;quot;217&amp;quot;    &amp;quot;218&amp;quot;    &amp;quot;219&amp;quot;    &amp;quot;2am&amp;quot;    &amp;quot;123&amp;quot;   
## [153] &amp;quot;220&amp;quot;    &amp;quot;1950s&amp;quot;  &amp;quot;2&amp;quot;      &amp;quot;20&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;19&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;22&amp;quot;    
## [161] &amp;quot;8&amp;quot;      &amp;quot;27&amp;quot;     &amp;quot;3&amp;quot;      &amp;quot;1,500&amp;quot;  &amp;quot;50&amp;quot;     &amp;quot;17&amp;quot;     &amp;quot;26&amp;quot;     &amp;quot;30&amp;quot;    
## [169] &amp;quot;10&amp;quot;     &amp;quot;70&amp;quot;     &amp;quot;6&amp;quot;      &amp;quot;3&amp;quot;      &amp;quot;4&amp;quot;      &amp;quot;30&amp;quot;     &amp;quot;33&amp;quot;     &amp;quot;45&amp;quot;    
## [177] &amp;quot;4&amp;quot;      &amp;quot;12&amp;quot;     &amp;quot;12&amp;quot;     &amp;quot;160&amp;quot;    &amp;quot;10&amp;quot;     &amp;quot;11&amp;quot;     &amp;quot;85&amp;quot;     &amp;quot;22&amp;quot;    
## [185] &amp;quot;11&amp;quot;     &amp;quot;10&amp;quot;     &amp;quot;50&amp;quot;     &amp;quot;300&amp;quot;    &amp;quot;30&amp;quot;     &amp;quot;10&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;440&amp;quot;   
## [193] &amp;quot;880&amp;quot;    &amp;quot;10&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;30&amp;quot;     &amp;quot;3000&amp;quot;   &amp;quot;3&amp;quot;      &amp;quot;3&amp;quot;      &amp;quot;3&amp;quot;     
## [201] &amp;quot;11&amp;quot;     &amp;quot;12&amp;quot;     &amp;quot;12&amp;quot;     &amp;quot;2&amp;quot;      &amp;quot;13&amp;quot;     &amp;quot;26&amp;quot;     &amp;quot;8&amp;quot;      &amp;quot;30&amp;quot;    
## [209] &amp;quot;11&amp;quot;     &amp;quot;30&amp;quot;     &amp;quot;19&amp;quot;     &amp;quot;7&amp;quot;      &amp;quot;30&amp;quot;     &amp;quot;8&amp;quot;      &amp;quot;30&amp;quot;     &amp;quot;28&amp;quot;    
## [217] &amp;quot;50&amp;quot;     &amp;quot;30&amp;quot;     &amp;quot;18&amp;quot;     &amp;quot;18&amp;quot;     &amp;quot;15&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;21&amp;quot;     &amp;quot;20&amp;quot;    
## [225] &amp;quot;6&amp;quot;      &amp;quot;30&amp;quot;     &amp;quot;19&amp;quot;     &amp;quot;16&amp;quot;     &amp;quot;2&amp;quot;      &amp;quot;23&amp;quot;     &amp;quot;25&amp;quot;     &amp;quot;35&amp;quot;    
## [233] &amp;quot;40&amp;quot;     &amp;quot;25&amp;quot;     &amp;quot;5&amp;quot;      &amp;quot;3&amp;quot;      &amp;quot;2&amp;quot;      &amp;quot;23&amp;quot;     &amp;quot;50&amp;quot;     &amp;quot;3&amp;quot;     
## [241] &amp;quot;3&amp;quot;      &amp;quot;1&amp;quot;      &amp;quot;2&amp;quot;      &amp;quot;2&amp;quot;      &amp;quot;23&amp;quot;     &amp;quot;40&amp;quot;     &amp;quot;35&amp;quot;     &amp;quot;8&amp;quot;     
## [249] &amp;quot;8&amp;quot;      &amp;quot;2&amp;quot;      &amp;quot;107th&amp;quot;  &amp;quot;16&amp;quot;     &amp;quot;27&amp;quot;     &amp;quot;10&amp;quot;     &amp;quot;8&amp;quot;      &amp;quot;60&amp;quot;    
## [257] &amp;quot;21&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;50&amp;quot;     &amp;quot;2&amp;quot;      &amp;quot;11&amp;quot;     &amp;quot;15&amp;quot;     &amp;quot;11&amp;quot;     &amp;quot;17&amp;quot;    
## [265] &amp;quot;17&amp;quot;     &amp;quot;2&amp;quot;      &amp;quot;34&amp;quot;     &amp;quot;45&amp;quot;     &amp;quot;49&amp;quot;     &amp;quot;52&amp;quot;     &amp;quot;55&amp;quot;     &amp;quot;3&amp;quot;     
## [273] &amp;quot;5&amp;quot;      &amp;quot;20&amp;quot;     &amp;quot;26&amp;quot;     &amp;quot;75.00&amp;quot;  &amp;quot;2&amp;quot;      &amp;quot;6&amp;quot;      &amp;quot;27&amp;quot;     &amp;quot;3&amp;quot;     
## [281] &amp;quot;4&amp;quot;      &amp;quot;00&amp;quot;     &amp;quot;1st&amp;quot;    &amp;quot;2nd&amp;quot;    &amp;quot;3rd&amp;quot;    &amp;quot;10&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;10&amp;quot;    
## [289] &amp;quot;30&amp;quot;     &amp;quot;50&amp;quot;     &amp;quot;50&amp;quot;     &amp;quot;11,000&amp;quot; &amp;quot;1&amp;quot;      &amp;quot;48th&amp;quot;   &amp;quot;4&amp;quot;      &amp;quot;6&amp;quot;     
## [297] &amp;quot;25th&amp;quot;   &amp;quot;100&amp;quot;    &amp;quot;100&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This works very nicely as well. However, to use these patterns with the tidyverse pipe, it is easier to use the &lt;em&gt;fitler()&lt;/em&gt; function rather than &lt;em&gt;str_subset()&lt;/em&gt;, and since it is convenient to chain steps in the pipe, we can use two calls to &lt;em&gt;filter()&lt;/em&gt;, first by underscores and secondly by digits. And as we don’t want either of these in our dataset, we will set the “negate” argument to TRUE in both cases. An alternative method to delete the digits would be to use the capital “D” in the regex, but this way keeps our filters more uniform, both with a “negate = TRUE” argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word %&amp;gt;%
  filter(str_detect(word, pattern = &amp;quot;_&amp;quot;, negate = TRUE)) %&amp;gt;%
  filter(str_detect(word, pattern = &amp;#39;[\\d]&amp;#39;, negate = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 17,953 x 3
##    sample        dream_number word      
##    &amp;lt;fct&amp;gt;                &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;     
##  1 college_women            1 dreamed   
##  2 college_women            1 office    
##  3 college_women            1 directress
##  4 college_women            1 nurses    
##  5 college_women            1 nursing   
##  6 college_women            1 school    
##  7 college_women            1 forty     
##  8 college_women            1 told      
##  9 college_women            1 results   
## 10 college_women            1 i.q       
## # … with 17,943 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-word-frequencies&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Plot word frequencies&lt;/h1&gt;
&lt;p&gt;Now we have done some tidying on the dataset, we can plot the word frequencies - a simple way is to pass them through a filter so we only retain those words with a frequency greater than say n = 60. Notice we use mutate to create the new variables for the plot &lt;em&gt;word&lt;/em&gt; (in the order of frequency) and &lt;em&gt;n&lt;/em&gt;. We then filter by frequency, and pass the two new variables to the ggplot function. We also have to switch syntax at this point from the pipe ( %&amp;gt;% ) to the + sign between the layers of the ggplot() function. We flip the coordinates, as it allows us to keep the words in the horizontal aspect and makes it the plot easier to read.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word %&amp;gt;%
  count(word, sort = TRUE) %&amp;gt;%
  mutate(word = reorder(word, n)) %&amp;gt;%
  filter(n &amp;gt; 60) %&amp;gt;%
  ggplot(aes(x = word, y = n)) +
  geom_col()+
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://drchristianryan.com/post/2020-01-14-manipulating-text-data-from-dreams_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This gives us an overview of the most commonly used words in dreams recalled by all four samples. But it would be more interesting to see how the word use differs between the samples. However, we should be prepared for the possibility that the length of dreams may vary between samples. To control for this, we might want to convert our raw counts of words to proportions from the dream text. Let’s check for the variety of dream lengths by using str_count() function on our original dataset df - hence before we removed our stopwords. We will count the words in each dream and store the result in a vector called dream_lengths. The default for &lt;em&gt;str_count&lt;/em&gt; is for the function to count characters if no pattern is given to match. However, if we pass it a second argument, specifying the regex for all sequences of non-space characters, it will count words instead. The regex includes the code for any non-white space character ‘\S’, with the addition of ‘+’ sign to indicate one or more non-white space characters, and the initial escape character ‘\’ as ‘\S’ is not recognised as an escape character without it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dream_lengths &amp;lt;- str_count(df$dream, &amp;quot;\\S+&amp;quot;)
plot(dream_lengths, xlab = &amp;quot;Dream Number&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://drchristianryan.com/post/2020-01-14-manipulating-text-data-from-dreams_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a good example of the use of the plot() function with a single vector in R. The default behaviour is to plot the values of the vector against the y-axis - dream_lengths in this case, and then use the index number (ie. the order in which each value occurs in the vector) as the x value. So our x-axis simple represents the order of the dreams, or as we have named this, the dream number. We can see here the range of dream lengths with the minimum being about 35 words and the maximum around 290 words. We could take the min, max, mean and SD if we wanted to be more specific.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;min(dream_lengths); max(dream_lengths); mean(dream_lengths); sd(dream_lengths)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 38&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 288&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 141.0325&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 45.09413&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is a great deal of variability in the dream lengths, so proportions will be better than raw counts to represent the frequency of each word.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;calculating-word-frequencies-as-proportions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Calculating word frequencies as proportions&lt;/h1&gt;
&lt;p&gt;We will want to count proportions after stopwords are removed. We have a choice here whether we want to express the frequency of individual words by proportion of a dream or proportion of a sample. These would have different interpretations. If the texts (in our case dreams) were much longer, proportion by text might be the better way to represent the data, but I suspect proportion by dream may not be very informative. Let’s try it and see what the results look like. We will &lt;em&gt;group_by()&lt;/em&gt; dream_number so as to create proportion by dream. Then we use a summarise function to create a word count, and we use mutate to convert this to percentage. I used a second mutate to clean this up into two decimal places with the &lt;em&gt;round()&lt;/em&gt; function. Finally, we use the tidyverse equivalent of &lt;em&gt;sort()&lt;/em&gt; which is the &lt;em&gt;arrange()&lt;/em&gt; function - but because we want this to be largest-to-smallest, we also include the &lt;em&gt;desc()&lt;/em&gt; descending function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word %&amp;gt;%
  group_by(dream_number, word) %&amp;gt;%
  summarise(n = n()) %&amp;gt;%
  mutate(percent = (n / sum(n))*100) %&amp;gt;%
  mutate(percent = round(percent, 2)) %&amp;gt;%
  arrange(desc(percent)) %&amp;gt;%
  ungroup&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 15,500 x 4
##    dream_number word         n percent
##           &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
##  1            6 remember     3    21.4
##  2          358 office       5    20  
##  3          355 dog          7    19.4
##  4          381 bus          6    18.2
##  5            2 hair         3    16.7
##  6           20 bed          3    16.7
##  7           83 store        5    16.7
##  8          260 car          6    16.7
##  9          399 test         6    15.8
## 10           13 dream        2    15.4
## # … with 15,490 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So in dream number 6 the word ‘remember’ accounted for 21% of the non-stopwords used. That seems like a high proportion. It might be more useful to look at the data aggregated across samples. We can change the code to group_by &lt;em&gt;sample&lt;/em&gt; instead of &lt;em&gt;dream_number&lt;/em&gt;, then recalculate the most frequently occurring words as a proportion of words by sample.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word %&amp;gt;%
  group_by(sample, word) %&amp;gt;%
  summarise(n = n()) %&amp;gt;%
  mutate(percent = (n / sum(n))*100) %&amp;gt;%
  mutate(percent = round(percent, 2)) %&amp;gt;%
  arrange(desc(percent)) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8,118 x 4
##    sample        word         n percent
##    &amp;lt;fct&amp;gt;         &amp;lt;chr&amp;gt;    &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
##  1 college_women remember    55    1.63
##  2 hall_male     dream       57    1.31
##  3 hall_male     car         51    1.17
##  4 hall_male     house       49    1.13
##  5 vietnam_vet   woman       64    1   
##  6 hall_female   remember    40    0.96
##  7 college_women car         32    0.95
##  8 college_women dream       32    0.95
##  9 hall_female   dream       38    0.92
## 10 hall_female   house       36    0.87
## # … with 8,108 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that for the college women, the word ‘remember’ features the most frequently across the whole sample of 100 dreams and makes up roughly 1.6% of the non-stopwords in the dreams recorded.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;We have explored how to tokenise texts, do some basic text cleaning and creating counts and proportions and finally graphed the simple word counts. In the next post in this series, I will explore the dream data using a clever technique from Julia Silge and David Robinson’s book that involves the &lt;em&gt;spread()&lt;/em&gt; and &lt;em&gt;gather()&lt;/em&gt; functions.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2.1 - Pulling text data from the internet</title>
      <link>https://drchristianryan.com/2019/11/30/pulling-text-data-from-the-internet/</link>
      <pubDate>Sat, 30 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://drchristianryan.com/2019/11/30/pulling-text-data-from-the-internet/</guid>
      <description>


&lt;p&gt;I have been working on the area of alexithymia for the last couple of years, a sub-clinical condition in which people find it difficult to identify and describe their emotions. I am currently analysing a dataset containing transcripts of interviews with people with and without alexithymia and I wanted to try out some R tools for text analysis. However, to do a blog post I needed some public data, and while mulling over which data I might use, I stumbled upon a line in &lt;strong&gt;“You are a thing and I love you”&lt;/strong&gt; - the wonderful new book on AI by Janelle Shane.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://drchristianryan.com/post/2019-11-30-pulling-text-data-from-the-internet_files/you_look.png&#34; style=&#34;width:30.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.amazon.com/You-Look-Like-Thing-Love/dp/0316525243&#34; class=&#34;uri&#34;&gt;https://www.amazon.com/You-Look-Like-Thing-Love/dp/0316525243&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;She mentions training an AI on a dream dataset available at &lt;a href=&#34;http://www.dreambank.net&#34; class=&#34;uri&#34;&gt;http://www.dreambank.net&lt;/a&gt; The website has section called “DreamBank” that allows you to search or take random samples of dreams recorded from a variety of sources. Under the Random Sample link, at: &lt;a href=&#34;http://www.dreambank.net/random_sample.cgi&#34; class=&#34;uri&#34;&gt;http://www.dreambank.net/random_sample.cgi&lt;/a&gt; one can select a dream source.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://drchristianryan.com/post/2019-11-30-pulling-text-data-from-the-internet_files/dreamselect.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will need a few packages for this process - &lt;strong&gt;rvest&lt;/strong&gt; is useful for pulling data from online sources. The two text packages &lt;strong&gt;stringr&lt;/strong&gt; and &lt;strong&gt;stringi&lt;/strong&gt; offer a range of tools for managing text data. The &lt;strong&gt;tidyverse&lt;/strong&gt; will simplify the management of the dataset and &lt;strong&gt;knitr&lt;/strong&gt; is useful for managing the display of text in Rmarkdown documents.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rvest)
library(stringr)
library(stringi)
library(tidyverse)
library(knitr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s start by taking a look at the dreams of college women from the 1940’s. We set an address for the url, then pass this as an argument to the read_html() function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;url &amp;lt;- &amp;quot;http://www.dreambank.net/random_sample.cgi?series=hall_female&amp;amp;min=100&amp;amp;max=300&amp;amp;n=100&amp;quot;
page &amp;lt;- read_html(url)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I followed the guidance in Kwartler (&lt;strong&gt;‘Text Mining in Practice with R’&lt;/strong&gt;, 2017) and checked the field with the dream text on the webpage in Chrome using the SelectorGadget plugin. This revealed that these text fields were labelled as “span”. So we can include this as the type of node to select in the html_node() function from &lt;strong&gt;rvest&lt;/strong&gt;. This allows us to pull the html text from just these fields and store them in a new variable called posts, then I will convert this html_text to raw text and store it in a variable called dream. I suppose I could have wrapped the html_nodes call within the html_text function and skipped creating an intermediate variable (posts), but I think it makes the code more readable this way.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts &amp;lt;- html_nodes(page, &amp;#39;span&amp;#39;)
dream &amp;lt;- html_text(posts)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can convert this to a dataframe - we will use the tidyverse version, a tibble, as this will avoid problems of the dreams being converted to factors. For more on why this can be problematic, read &lt;strong&gt;“stringsAsFactors: An unauthorized biography”&lt;/strong&gt; by Roger Peng at this site:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://simplystatistics.org/2015/07/24/stringsasfactors-an-unauthorized-biography/&#34; class=&#34;uri&#34;&gt;https://simplystatistics.org/2015/07/24/stringsasfactors-an-unauthorized-biography/&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- tibble(dream = dream)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a side-note, we could have done each of these steps with a more tidyverse syntax, by using the pipe, though this may have meant that each of the substeps was less transparent. We could have taken the pages object that contains
our raw data and piped it through the various functions to extract just the dreams, then converted it into a tibble. As we haven’t declared a name for the one variable in the tibble, we need to use the rename function to assign the name ‘dream’ to the column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- page %&amp;gt;% 
  html_nodes(&amp;#39;span&amp;#39;) %&amp;gt;% 
  html_text() %&amp;gt;% 
  tibble() %&amp;gt;% 
  rename(&amp;#39;dream&amp;#39; = &amp;#39;.&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a quick peak at the data. We can create a quick function to truncate the display of the dreams to 60 characters. We will call this function custom_view(). We will restrict the view to just the first 5 rows as well, using indexing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;custom_view &amp;lt;- function(x) data.frame(lapply(x, substr, 1, 60))
custom_view(df[1:5,])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                                           dream
## 1 \n#0001 (Code 001, Age 24, 11/??/47)I dreamed that I was at a
## 2 \n#0008 (Code 001, Age 24, 11/??/47)I dreamed that I went to 
## 3 \n#0025 (Code 007, Age 20, 03/20/48)As I first remember the d
## 4 \n#0028 (Code 007, Age 20, 04/09/48)I was at a factory workin
## 5 \n#0036 (Code 008, Age 22, 02/25/48)I was in a house. It was&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Currently, our dataset has just one column and we will need to fix this. Let’s use substr to pull out the dream number that occurs at the beginning of the text field. The substr() function takes three arguments, the vector, character start and character stop.&lt;/p&gt;
&lt;p&gt;For example, this is what we get if we pull out the three numeric identifier characters (start at 4 and stop at 6).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;substr(df$dream, 4, 6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [1] &amp;quot;001&amp;quot; &amp;quot;008&amp;quot; &amp;quot;025&amp;quot; &amp;quot;028&amp;quot; &amp;quot;036&amp;quot; &amp;quot;038&amp;quot; &amp;quot;049&amp;quot; &amp;quot;050&amp;quot; &amp;quot;051&amp;quot; &amp;quot;055&amp;quot; &amp;quot;059&amp;quot; &amp;quot;065&amp;quot;
##  [13] &amp;quot;074&amp;quot; &amp;quot;081&amp;quot; &amp;quot;084&amp;quot; &amp;quot;087&amp;quot; &amp;quot;098&amp;quot; &amp;quot;101&amp;quot; &amp;quot;117&amp;quot; &amp;quot;123&amp;quot; &amp;quot;137&amp;quot; &amp;quot;156&amp;quot; &amp;quot;163&amp;quot; &amp;quot;164&amp;quot;
##  [25] &amp;quot;165&amp;quot; &amp;quot;186&amp;quot; &amp;quot;190&amp;quot; &amp;quot;221&amp;quot; &amp;quot;223&amp;quot; &amp;quot;225&amp;quot; &amp;quot;226&amp;quot; &amp;quot;230&amp;quot; &amp;quot;232&amp;quot; &amp;quot;251&amp;quot; &amp;quot;261&amp;quot; &amp;quot;271&amp;quot;
##  [37] &amp;quot;278&amp;quot; &amp;quot;290&amp;quot; &amp;quot;292&amp;quot; &amp;quot;296&amp;quot; &amp;quot;301&amp;quot; &amp;quot;302&amp;quot; &amp;quot;304&amp;quot; &amp;quot;309&amp;quot; &amp;quot;310&amp;quot; &amp;quot;313&amp;quot; &amp;quot;335&amp;quot; &amp;quot;351&amp;quot;
##  [49] &amp;quot;352&amp;quot; &amp;quot;353&amp;quot; &amp;quot;356&amp;quot; &amp;quot;360&amp;quot; &amp;quot;361&amp;quot; &amp;quot;367&amp;quot; &amp;quot;368&amp;quot; &amp;quot;371&amp;quot; &amp;quot;373&amp;quot; &amp;quot;383&amp;quot; &amp;quot;384&amp;quot; &amp;quot;396&amp;quot;
##  [61] &amp;quot;402&amp;quot; &amp;quot;404&amp;quot; &amp;quot;405&amp;quot; &amp;quot;406&amp;quot; &amp;quot;408&amp;quot; &amp;quot;415&amp;quot; &amp;quot;435&amp;quot; &amp;quot;440&amp;quot; &amp;quot;460&amp;quot; &amp;quot;468&amp;quot; &amp;quot;475&amp;quot; &amp;quot;483&amp;quot;
##  [73] &amp;quot;491&amp;quot; &amp;quot;499&amp;quot; &amp;quot;503&amp;quot; &amp;quot;506&amp;quot; &amp;quot;528&amp;quot; &amp;quot;530&amp;quot; &amp;quot;537&amp;quot; &amp;quot;540&amp;quot; &amp;quot;543&amp;quot; &amp;quot;547&amp;quot; &amp;quot;550&amp;quot; &amp;quot;556&amp;quot;
##  [85] &amp;quot;563&amp;quot; &amp;quot;582&amp;quot; &amp;quot;586&amp;quot; &amp;quot;606&amp;quot; &amp;quot;609&amp;quot; &amp;quot;616&amp;quot; &amp;quot;620&amp;quot; &amp;quot;629&amp;quot; &amp;quot;641&amp;quot; &amp;quot;652&amp;quot; &amp;quot;659&amp;quot; &amp;quot;660&amp;quot;
##  [97] &amp;quot;664&amp;quot; &amp;quot;666&amp;quot; &amp;quot;667&amp;quot; &amp;quot;681&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This worked fine, so let’s create a new variable called code to store this data in our dataframe. This will be our id code for each dream.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df$code &amp;lt;- substr(df$dream, 4, 6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The column with the code should come first, so we will swap the order of columns with a simple index call - concatenating the order of variables, passed as the second argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- df[,c(2,1)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After examining the dataframe, we can see that the pattern for ages is given by the word ‘Age’ with a capital ‘A’, followed by a space, then the actual age as two digits, like this: “Age 24”. We can create a regex pattern to match this and use the stringr package to extract this string and store it in a vector called age.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;age &amp;lt;- str_extract(df$dream, &amp;quot;[A][g][e][ ][0-9]{2}&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, if we want to manipulate the ages as integers, we need to extract just the number and coerce it from a character vector into a numeric vector. We can do this with another regex, which just pulls out the two digits. And let’s convert it into a numeric and paste the data back into the dataframe, and move it to the second column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;age_refined &amp;lt;- str_extract(age, &amp;quot;([0-9]{2})&amp;quot;)
df$age &amp;lt;- as.numeric(age_refined)
df &amp;lt;- df[,c(1,3,2)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we want to tidy up the dream variable. At the moment we have a bunch of characters before the dream itself starts. We can experiment with the str_locate function and a regex to see if we can identify the pattern for where the dream begins. Let’s try the closing brace which seems to come after the date of the dream.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(str_locate(df$dream, &amp;quot;[)]&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      start end
## [1,]    35  35
## [2,]    35  35
## [3,]    35  35
## [4,]    35  35
## [5,]    35  35
## [6,]    35  35&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This indicates that a closing brace always occurs at the 35th character in the dream text field. We can use the Base R function substr() which takes a vector, a start and an end point. We know the start (character 36), which is the first character after the closing brace of the date, but we don’t know the end, as all the dreams are different lenghts. But we can use the handy nchar() function which
counts the number of characters for us, so we treat this as a flexible endpoint. As this seems to work nicely, let’s overwrite our dream variable with this new version&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df$dream &amp;lt;- substr(df$dream, 36, nchar(df$dream))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A quick look at the df using our custom_view() function indictes this is shaping up nicely.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;custom_view(df)[1:5,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   code age                                                        dream
## 1  001  24 I dreamed that I was at a public affair but I don&amp;#39;t know whi
## 2  008  24 I dreamed that I went to take an examination and I was late 
## 3  025  20 As I first remember the dream I was upstairs in a room with 
## 4  028  20 I was at a factory working. I saw a college girl-friend of m
## 5  036  22 I was in a house. It was a beautiful large home with expensi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But what about the end of each dream? Let’s examine the first dream in detail.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;I dreamed that I was at a public affair but I don&amp;#39;t know which affair&amp;quot;
##  [2] &amp;quot;it was although it was outdoors. There were many people around us and&amp;quot;
##  [3] &amp;quot;they were of all ages. I was at this affair with B. He is about&amp;quot;      
##  [4] &amp;quot;twenty-six years old and he is the boy-friend of one of the girls&amp;quot;    
##  [5] &amp;quot;that lives in the dormitory that I do. Whenever I felt the urge to&amp;quot;   
##  [6] &amp;quot;get away from my escort or from the people at the affair, I would&amp;quot;    
##  [7] &amp;quot;start to fly. (like superman). While up in the air I felt very uneasy&amp;quot;
##  [8] &amp;quot;and worried about how I would get back down without hurting myself. I&amp;quot;
##  [9] &amp;quot;left my escort about three times in this way. I do not remember why I&amp;quot;
## [10] &amp;quot;felt that I had to get away. Interpretation I do not know why I would&amp;quot;
## [11] &amp;quot;dream of B. I do not know him very well and I do not feel very&amp;quot;       
## [12] &amp;quot;friendly toward him when I do see him. I believe that I associated&amp;quot;   
## [13] &amp;quot;him with my studies and felt that I had to get away for a short&amp;quot;      
## [14] &amp;quot;while. When I had this dream, I hadn&amp;#39;t been home for about eight&amp;quot;     
## [15] &amp;quot;weeks and was looking forward to going home. I felt that I wanted a&amp;quot;  
## [16] &amp;quot;short vacation from my studies and this dream was an escape mechanism&amp;quot;
## [17] &amp;quot;in the form of a fantasy to get away from my classes for a short&amp;quot;     
## [18] &amp;quot;while. Answers to questions 2. Frustrated. I felt that I had to get&amp;quot;  
## [19] &amp;quot;away.3. actual participant4. unpleasant5. Vague, but it was&amp;quot;          
## [20] &amp;quot;outdoors.6. No.7. No. (268 words)&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that each dream includes an interpretation and I only want to analyse the dream narrative itself, not the person’s reflections on the meaning of the dream. We can use the word ‘Interpretation’ to identify the end point of the dream narrative. We can just pull out the first 6 values by wrapping this in the head function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(str_locate(df$dream, &amp;quot;Interpretation&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      start end
## [1,]   643 656
## [2,]   722 735
## [3,]   642 655
## [4,]   306 319
## [5,]   415 428
## [6,]   327 340&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We still need to do a bit of work - the str_locate() returns two values and we only want the first one. Secondly, when we trim the text, we want to start two characters to the left as we don’t want the first letter of the word “Interpretation”, or the whitespace just before it. We can store the location in a new vector called loc - then we can take out the start point only, with the index [,1]. On the third line we will crop the text to start at 0 and end at 2 characters to the left (-2) of the start point. We reassign it to the same variable in our dataframe - df$dream.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loc &amp;lt;- str_locate(df$dream, &amp;quot;Interpretation&amp;quot;)
start &amp;lt;- loc[,1] # take out start point [,1] as a vector called start
df$dream &amp;lt;- substr(df$dream, 0, start-2) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, let’s check that the changes worked by examining the final 70 characters of the first dream.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;is way. I do not remember why I felt that I had to get away.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is just what we wanted - this is line 9 and 10 of the full dream we examined up above - finishing just before the interpretation starts.&lt;/p&gt;
&lt;p&gt;In the next post, I will pull in the dreams from three other samples and start to look at the sentiment analysis of the dream content.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>1 - Plotting multiple histograms on the same graph</title>
      <link>https://drchristianryan.com/2019/10/12/plotting-multiple-histograms-on-the-same-graph/</link>
      <pubDate>Sat, 12 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://drchristianryan.com/2019/10/12/plotting-multiple-histograms-on-the-same-graph/</guid>
      <description>


&lt;p&gt;Recently, while trying to compare the distribution of two samples, I discovered that you can plot both on the same graph in base R, which is a nice feature if you just want to examine the data quickly. We can explore this with a psychological dataset from the Open Psychometrics site. This hosts a range of open psychometric tests and stores the data in an accessible form. Let’s pull out the data for the Rosenberg Self-Esteem Scale (note that there are two different scoring methods in common use on this scale - on the website they have used a 1 - 4 Likert scale for the data output as a csv, but it is not unusual to see the use of a 0 - 3 scale, (which is the method used to give participants on the website feedback) so we need to be cautious when comparing these total scores with published norms (see &lt;a href=&#34;https://socy.umd.edu/about-us/using-rosenberg-self-esteem-scale&#34; class=&#34;uri&#34;&gt;https://socy.umd.edu/about-us/using-rosenberg-self-esteem-scale&lt;/a&gt;)).&lt;/p&gt;
&lt;p&gt;First we will load two packages we are going to use. We want the &lt;strong&gt;tidyverse&lt;/strong&gt; for manipulating the variables and we will use the &lt;strong&gt;psych&lt;/strong&gt; package for creating total scores on the measure itself.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(psych)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we want to set an url object to direct the download.file() to the right place to pull the data. I have called it &lt;em&gt;my_url&lt;/em&gt; for simplicity. We pass this as the first argument in the download.file() function. We then set a destination for the file to be saved with the &lt;em&gt;dest&lt;/em&gt; argument. Finally we use unzip to unpack the zipped file.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_url &amp;lt;- &amp;quot;http://openpsychometrics.org/_rawdata/RSE.zip&amp;quot;
download.file(url = my_url, dest=&amp;quot;data.zip&amp;quot;, mode=&amp;quot;wb&amp;quot;) 
unzip (&amp;quot;data.zip&amp;quot;, exdir = &amp;quot;./&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can import the data with the read_tsv() function. We can’t use the read_csv() function with the data, because despite having a .csv extension, the data is actually tab-separated not comma-separated.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- read_tsv(&amp;quot;RSE/data.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the Rosenberg Self-Esteem scale Items 2, 5, 6, 8, 9 are normally reverse scored. However, whoever loaded the questions on the website put them in a different order, with items 3, 5, 8, 9, 10 needing reversing. We need to create a total score for the measure and to be mindful of the reverse coded items. The &lt;em&gt;psych&lt;/em&gt; package provides a function for this called scoreFast. We need to pass it a list called keys.list which specifies the direction of each item in turn (items are scored as-is if they have no leading ‘-’ minus sign, but all items with a minus are reverse scored). We won’t bother recoding the data from the 1 - 4 scale to 0 - 3 as it makes little difference for your graphs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;keys.list &amp;lt;- list(c(&amp;#39;Q1&amp;#39;, &amp;#39;Q2&amp;#39;, &amp;#39;-Q3&amp;#39;, &amp;#39;Q4&amp;#39;, &amp;#39;-Q5&amp;#39;, &amp;#39;Q6&amp;#39;, &amp;#39;Q7&amp;#39;, &amp;#39;-Q8&amp;#39;, &amp;#39;-Q9&amp;#39;, &amp;#39;-Q10&amp;#39;))
df$total &amp;lt;- scoreFast(keys.list, items = df[1:10], totals = TRUE, min = 1, max = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have our dataset, we can look at comparing distributions. We might want to know if the distribution of self-esteem scores differs between men and women. Checking the codebook on the website, we can see that males are coded as ‘1’ and females as ‘2’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;men &amp;lt;- df %&amp;gt;% 
  filter(gender == 1)
women &amp;lt;- df %&amp;gt;% 
  filter(gender == 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So let’s plot the total self-esteem scores for the women in the sample as a simple histogram.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(women$total)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://drchristianryan.com/post/2019-10-12-plotting-multiple-histograms-on-the-same-graph_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;
We can see a fairly normal distribution of scores. We can check the mean, but we might predict it is around 25.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(women$total)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 25.74368&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we can add the men’s scores to the same plot. Here we simply create the first plot, then make a second plot with the argument &lt;strong&gt;add&lt;/strong&gt; set to TRUE. We will set the density to 35 so we can see through the bars on the histogram.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(women$total, col = &amp;#39;red&amp;#39;, main = &amp;quot;Histogram of Total scores on Rosenberg Self-Esteem Scale&amp;quot;, xlab = &amp;quot;Total score&amp;quot;)
hist(men$total, add = TRUE, col = &amp;#39;blue&amp;#39;, density = 35)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://drchristianryan.com/post/2019-10-12-plotting-multiple-histograms-on-the-same-graph_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I have used the pipe to separate my data into individual gender dataframes, but this is only one way to do it, and I do find this code very easy to read. However, we could have done the same thing using a traditional R approach of indexing instead.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(df$total[df$gender== 2], col = &amp;#39;orchid&amp;#39;, main = &amp;quot;Histogram of Total scores on Rosenberg Self-Esteem Scale&amp;quot;, xlab = &amp;quot;Total score&amp;quot;)
hist(df$total[df$gender==1], add = TRUE, col = &amp;#39;royalblue&amp;#39;, density = 40)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://drchristianryan.com/post/2019-10-12-plotting-multiple-histograms-on-the-same-graph_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we have seen the distributions, we might wonder if the sexes differ on the measure of self-esteem. Let’s run a quick t-test to see.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(men$total, women$total)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  men$total and women$total
## t = 23.785, df = 37496, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  1.436304 1.694284
## sample estimates:
## mean of x mean of y 
##  27.30897  25.74368&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yes they do! With men having a significantly higher mean score on self-esteem (though the absolute difference is quite small.)&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
